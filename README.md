# ğŸ¤– AI-VIEW: AI Vision Assistant for Smart Glasses

> A multi-modal AI-powered assistant built for smart glasses â€” combining computer vision, natural language understanding, and gesture recognition into a unified experience.

**AI-VIEW** enables real-time awareness and interaction through sight, voice, emotion, and gestures â€” tailored for accessibility, mobility, and human-AI augmentation.

---

## ğŸ§  Core Features

- ğŸ¥½ **Walking Mode**  
  Detects objects (0â€“15m range) and estimates distance using color-coded alerts.  
  â†’ Powered by YOLOv8 and custom proximity mapping.

- ğŸ—£ï¸ **Talking Mode**  
  Engage in AI conversations with natural speech using OpenAI and ElevenLabs APIs.  
  â†’ Arabic + English support.

- ğŸ˜Š **Emotion Detection Mode**  
  Detects facial expressions (happy, sad, neutral, etc.) in real time.  
  â†’ Based on DeepFace.

- ğŸ“– **Reading Mode**  
  Recognizes text in English and Arabic using OCR, then reads it aloud.  
  â†’ EasyOCR + multilingual TTS.

- âœ‹ **Gesture Mode**  
  Recognizes hand signs using MediaPipe for intuitive control (e.g., stop, thumbs up, point).

---

## ğŸ§© Architecture Overview

```plaintext
ğŸ“¦ ai_vision_assistant/
â”œâ”€â”€ ai.py                 # Main controller
â”œâ”€â”€ voice.py              # Voice and TTS handling
â”œâ”€â”€ .env.template         # API key structure
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ README.md             # You're here!
